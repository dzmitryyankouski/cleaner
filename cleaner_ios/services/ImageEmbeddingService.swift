import Foundation
@preconcurrency import Vision
import CoreML
import UIKit
import Photos
import NaturalLanguage
import CoreVideo

class ImageEmbeddingService {
    private var mobileClipImageModel: Any?
    private var mobileClipTextModel: Any?
    private var concurrentTasks = 10

    var tokenizer: CLIPTokenizer?
    
    init() {
        loadImageModel()
        loadTextModel()
        loadTokenizer()
    }

    func switchModel(model: String) {
        do {
            switch model {
                case "s0":
                    mobileClipImageModel = try mobileclip_s0_image()
                    mobileClipTextModel = try mobileclip_s0_text()
                case "s1":
                    mobileClipImageModel = try mobileclip_s1_image()
                    mobileClipTextModel = try mobileclip_s1_text()
                case "s2":
                    mobileClipImageModel = try mobileclip_s2_image()
                    mobileClipTextModel = try mobileclip_s2_text()
                default:
                    mobileClipImageModel = try mobileclip_s0_image()
                    mobileClipTextModel = try mobileclip_s0_text()
            }
            print("‚úÖ –ú–æ–¥–µ–ª—å \(model) —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ \(model): \(error)")
        }
    }
    
    private func loadTokenizer() {
        do {
            tokenizer = try CLIPTokenizer()
            print("‚úÖ CLIP —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω!")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: \(error)")
            tokenizer = nil
        }
    }
    
    private func loadImageModel() {
        do {
            mobileClipImageModel = try mobileclip_s0_image()
            print("Image Model loaded")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ Bundle: \(error)")
        }
    }
    
    private func loadTextModel() {
        do {
            mobileClipTextModel = try mobileclip_s0_text()
            print("Text Model loaded")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Bundle: \(error)")
        }
    }

    func indexPhotos(assets: [PHAsset], onItemCompleted: ((Int, [Float]) -> Void)? = nil) async {
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é \(assets.count) —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π...")

        var embeddings: [[Float]] = []
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∞—Å—Å–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ç–∞—Å–∫–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        await withTaskGroup(of: (Int, [Float]?)?.self) { group in
            var activeTasks = 0
            
            for (index, asset) in assets.enumerated() {
                // –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–æ –¥–ª—è –Ω–æ–≤–æ–π —Ç–∞—Å–∫–∏
                while activeTasks >= concurrentTasks {
                    if let result = await group.next() {
                        if let (assetIndex, embedding) = result, let embedding = embedding {
                            embeddings.append(embedding)

                            await MainActor.run {
                                onItemCompleted?(assetIndex, embedding)
                            }
                        }
                        activeTasks -= 1
                    }
                }
                
                group.addTask {
                    let embedding = await self.processSingleAsset(asset)
                    return (index, embedding)
                }
                activeTasks += 1
            }
            
            // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for await result in group {
                if let (assetIndex, embedding) = result, let embedding = embedding {
                    // –û–±–Ω–æ–≤–ª—è–µ–º UI –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                    await MainActor.run {
                        embeddings.append(embedding)
                        onItemCompleted?(assetIndex, embedding)
                    }
                }
            }
        }
    }

    func generateEmbedding(from pixelBuffer: CVPixelBuffer) async -> [Float] {
        do {
            guard let mobileClipImageModel = mobileClipImageModel else {
                print("‚ùå Image model not loaded")
                return []
            }
        
            // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏ –≤—ã–∑—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ prediction
            if let s0Model = mobileClipImageModel as? mobileclip_s0_image {
                let output = try await s0Model.prediction(image: pixelBuffer)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else if let s1Model = mobileClipImageModel as? mobileclip_s1_image {
                let output = try await s1Model.prediction(image: pixelBuffer)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else if let s2Model = mobileClipImageModel as? mobileclip_s2_image {
                let output = try await s2Model.prediction(image: pixelBuffer)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else {
                print("‚ùå Unknown image model type")
                return []
            }
        } catch {
            print("‚ùå Error: \(error)")
            return []
        }
    }

    func generateEmbeddings(from pixelBuffers: [CVPixelBuffer]) async -> [[Float]] {
        var results: [[Float]] = []
        
        for pixelBuffer in pixelBuffers {
            let embedding = await generateEmbedding(from: pixelBuffer)
            results.append(embedding)
        }
        
        return results
    }

    func textToEmbedding(text: String) async -> [Float] {
        do {
            guard let tokenizer = tokenizer else {
                print("‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
                return []
            }

            guard let model = mobileClipTextModel else {
                print("‚ùå Text model not loaded")
                return []
            }

            let inputIds = tokenizer.encode_full(text: text)
                
            let inputArray = try MLMultiArray(shape: [1, 77], dataType: .int32)

            for (index, element) in inputIds.enumerated() {
                inputArray[index] = NSNumber(value: element)
            }

            // –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –∏ –≤—ã–∑—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ prediction
            if let s0Model = model as? mobileclip_s0_text {
                let output = try s0Model.prediction(text: inputArray)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else if let s1Model = model as? mobileclip_s1_text {
                let output = try s1Model.prediction(text: inputArray)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else if let s2Model = model as? mobileclip_s2_text {
                let output = try s2Model.prediction(text: inputArray)
                return convertMultiArrayToFloatArray(output.final_emb_1)
            } else {
                print("‚ùå Unknown text model type")
                return []
            }
        } catch {
            print(error.localizedDescription)
            return []
        }
    }
    
    /// –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç -1 –¥–æ 1, –≥–¥–µ 1 –æ–∑–Ω–∞—á–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    func cosineSimilarity(_ embedding1: [Float], _ embedding2: [Float]) -> Float {
        guard embedding1.count == embedding2.count else {
            print("‚ùå Embeddings must have the same dimension")
            return 0.0
        }
        
        var dotProduct: Float = 0.0
        var norm1: Float = 0.0
        var norm2: Float = 0.0
        
        for i in 0..<embedding1.count {
            dotProduct += embedding1[i] * embedding2[i]
            norm1 += embedding1[i] * embedding1[i]
            norm2 += embedding2[i] * embedding2[i]
        }
        
        let magnitude = sqrt(norm1) * sqrt(norm2)
        guard magnitude > 0 else {
            return 0.0
        }
        
        return dotProduct / magnitude
    }
    
    /// –ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    func findSimilarPhotos(query: String, minSimilarity: Float = 0.14, photos: [Photo]) async -> [(Photo, Float)] {
        let queryEmbedding = await textToEmbedding(text: query)
        
        guard !queryEmbedding.isEmpty else {
            print("‚ùå Failed to generate query embedding")
            return []
        }
        
        var similarities: [(Photo, Float)] = []
        
        for photo in photos {
            let similarity = cosineSimilarity(queryEmbedding, photo.embedding)
            similarities.append((photo, similarity))
        }
        
        // –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –ø–æ—Ä–æ–≥—É —Å—Ö–æ–¥—Å—Ç–≤–∞ (14%) –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é
        let filteredResults = similarities
            .filter { $0.1 >= minSimilarity }
            .sorted { $0.1 > $1.1 }
        
        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–Ω–æ –Ω–µ –±–æ–ª—å—à–µ —á–µ–º –µ—Å—Ç—å –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
        return Array(filteredResults)
    }

    private func processSingleAsset(_ asset: PHAsset) async -> [Float]? {
        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—Å—Å–µ—Ç –≤ CVPixelBuffer
        guard let pixelBuffer = await convertAssetToThumbnail(asset) else {
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å CVPixelBuffer –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
            return nil
        }
        
        let embedding = await generateEmbedding(from: pixelBuffer)
        
        if embedding.isEmpty {
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–∏–Ω–≥ –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
            return nil
        }

        return embedding
    }

    private func convertMultiArrayToFloatArray(_ multiArray: MLMultiArray) -> [Float] {
        let count = multiArray.count
        var result = [Float](repeating: 0, count: count)
        
        for i in 0..<count {
            result[i] = Float(truncating: multiArray[i])
        }
        
        return result
    }

    private func convertAssetToThumbnail(_ asset: PHAsset) async -> CVPixelBuffer? {
        return await withCheckedContinuation { continuation in
            let imageManager = PHImageManager.default()
            let requestOptions = PHImageRequestOptions()
            
            requestOptions.isSynchronous = false
            requestOptions.deliveryMode = .highQualityFormat
            requestOptions.resizeMode = .exact
            requestOptions.isNetworkAccessAllowed = false
            
            // –†–∞–∑–º–µ—Ä –º–∏–Ω–∏–∞—Ç—é—Ä—ã –¥–ª—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (MobileCLIP —Ç—Ä–µ–±—É–µ—Ç 256x256)
            let targetSize = CGSize(width: 256, height: 256)
            
            imageManager.requestImage(
                for: asset,
                targetSize: targetSize,
                contentMode: .aspectFill,
                options: requestOptions
            ) { image, info in
                guard let image = image,
                      let cgImage = image.cgImage else {
                    continuation.resume(returning: nil)
                    return
                }
                
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º CVPixelBuffer —Ç–æ—á–Ω–æ 256x256
                print("üñºÔ∏è –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: \(cgImage.width)x\(cgImage.height)")
                let pixelBuffer = self.cgImageToPixelBuffer(cgImage)
                if let buffer = pixelBuffer {
                    print("‚úÖ CVPixelBuffer —Å–æ–∑–¥–∞–Ω: \(CVPixelBufferGetWidth(buffer))x\(CVPixelBufferGetHeight(buffer))")
                }
                continuation.resume(returning: pixelBuffer)
            }
        }
    }
    
    private func cgImageToPixelBuffer(_ cgImage: CGImage) -> CVPixelBuffer? {
        // MobileCLIP —Ç—Ä–µ–±—É–µ—Ç —Ç–æ—á–Ω–æ 256x256
        let targetWidth = 256
        let targetHeight = 256
        
        var pixelBuffer: CVPixelBuffer?
        let status = CVPixelBufferCreate(
            kCFAllocatorDefault,
            targetWidth,
            targetHeight,
            kCVPixelFormatType_32ARGB,
            nil,
            &pixelBuffer
        )
        
        guard status == kCVReturnSuccess, let buffer = pixelBuffer else {
            return nil
        }
        
        CVPixelBufferLockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0))
        defer { CVPixelBufferUnlockBaseAddress(buffer, CVPixelBufferLockFlags(rawValue: 0)) }
        
        let pixelData = CVPixelBufferGetBaseAddress(buffer)
        let rgbColorSpace = CGColorSpaceCreateDeviceRGB()
        
        guard let context = CGContext(
            data: pixelData,
            width: targetWidth,
            height: targetHeight,
            bitsPerComponent: 8,
            bytesPerRow: CVPixelBufferGetBytesPerRow(buffer),
            space: rgbColorSpace,
            bitmapInfo: CGImageAlphaInfo.noneSkipFirst.rawValue
        ) else {
            return nil
        }
        
        // –†–∏—Å—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–æ —Ç–æ—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ 256x256
        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: targetWidth, height: targetHeight))
        
        return buffer
    }
}