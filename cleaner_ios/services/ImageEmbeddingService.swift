import Foundation
@preconcurrency import Vision
import CoreML
import UIKit
import Photos
import NaturalLanguage

struct Photo {
    let asset: PHAsset
    let embedding: [Float]
}

class ImageEmbeddingService {
    private var mobileClipImageModel: MLModel?
    private var mobileClipTextModel: mobileclip_s0_text?
    private var concurrentTasks = 5

    var processedPhotos: [Photo] = []
    var tokenizer: CLIPTokenizer?
    
    // Callback –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö
    var onPhotoProcessed: ((Photo) -> Void)?
    var onIndexingComplete: (() -> Void)?
    
    init() {
        loadImageModel()
        loadTextModel()
        loadTokenizer()
    }
    
    private func loadTokenizer() {
        do {
            tokenizer = try CLIPTokenizer()
            print("‚úÖ CLIP —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω!")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: \(error)")
            tokenizer = nil
        }
    }
    
    private func loadImageModel() {
        if let modelURL = Bundle.main.url(forResource: "mobileclip_s0_image", withExtension: "mlmodelc") {
            do {
                mobileClipImageModel = try MLModel(contentsOf: modelURL)
                print("‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π MobileCLIP –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Bundle!")
                return
            } catch {
                print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ Bundle: \(error)")
            }
        }
    }
    
    private func loadTextModel() {
        do {
            mobileClipTextModel = try mobileclip_s0_text()
            print("Model loaded")
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Bundle: \(error)")
        }
    }

    func indexPhotos(photos: [PHAsset]) async {
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é \(photos.count) —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π...")
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∞—Å—Å–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ç–∞—Å–∫–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        await withTaskGroup(of: Photo?.self) { group in
            var activeTasks = 0
            
            for asset in photos {
                // –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–æ –¥–ª—è –Ω–æ–≤–æ–π —Ç–∞—Å–∫–∏
                while activeTasks >= concurrentTasks {
                    if let result = await group.next() {
                        if let photo = result {
                            self.processedPhotos.append(photo)
                            // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                            await MainActor.run {
                                self.onPhotoProcessed?(photo)
                            }
                        }
                        activeTasks -= 1
                    }
                }
                
                group.addTask {
                    await self.processSingleAsset(asset)
                }
                activeTasks += 1
            }
            
            // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for await result in group {
                if let photo = result {
                    // –û–±–Ω–æ–≤–ª—è–µ–º UI –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                    await MainActor.run {
                        self.processedPhotos.append(photo)
                        // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                        self.onPhotoProcessed?(photo)
                    }
                }
            }
        }
        
        // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        await MainActor.run {
            self.onIndexingComplete?()
        }
    }

    func generateEmbedding(from image: UIImage) async -> [Float] {
        guard let mobileClipImageModel = mobileClipImageModel else {
            print("‚ùå Image model not loaded")
            return []
        }

        guard let cgImage = image.cgImage else {
            print("‚ùå Error getting cgImage")
            return []
        }

        return await withCheckedContinuation { continuation in
            let request = VNCoreMLRequest(model: try! VNCoreMLModel(for: mobileClipImageModel)) { request, error in
                if let error = error {
                    print("‚ùå Error: \(error)")
                    continuation.resume(returning: [])
                    return
                }
                
                guard let results = request.results as? [VNCoreMLFeatureValueObservation],
                      let firstResult = results.first,
                      let multiArray = firstResult.featureValue.multiArrayValue else {
                    print("‚ùå Error getting embedding")
                    continuation.resume(returning: [])
                    return
                }
                
                let result = self.convertMultiArrayToFloatArray(multiArray)
                continuation.resume(returning: result)
            }
            
            request.imageCropAndScaleOption = VNImageCropAndScaleOption.centerCrop

            let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

            DispatchQueue.global(qos: .userInitiated).async {
                do {
                    try handler.perform([request])
                } catch {
                    print("‚ùå Error: \(error)")
                    continuation.resume(returning: [])
                }
            }
        }
    }

    func generateEmbeddings(from images: [UIImage]) async -> [[Float]] {
        var results: [[Float]] = []
        
        for image in images {
            let embedding = await generateEmbedding(from: image)
            results.append(embedding)
        }
        
        return results
    }

    func textToEmbedding(text: String) async -> [Float] {
        do {
            guard let tokenizer = tokenizer else {
                print("‚ùå –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")
                return []
            }

            guard let model = mobileClipTextModel else {
                print("‚ùå Text model not loaded")
                return []
            }

            let inputIds = tokenizer.encode_full(text: text)
                
            let inputArray = try MLMultiArray(shape: [1, 77], dataType: .int32)

            for (index, element) in inputIds.enumerated() {
                inputArray[index] = NSNumber(value: element)
            }

            let output = try model.prediction(text: inputArray).final_emb_1
            
            let count = output.count
            var result = [Float](repeating: 0, count: count)

            for i in 0..<count {
                result[i] = Float(truncating: output[i])
            }

            return result
        } catch {
            print(error.localizedDescription)
            return []
        }
    }
    
    /// –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è —ç–º–±–µ–¥–∏–Ω–≥–∞–º–∏
    /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç -1 –¥–æ 1, –≥–¥–µ 1 –æ–∑–Ω–∞—á–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    func cosineSimilarity(_ embedding1: [Float], _ embedding2: [Float]) -> Float {
        guard embedding1.count == embedding2.count else {
            print("‚ùå Embeddings must have the same dimension")
            return 0.0
        }
        
        var dotProduct: Float = 0.0
        var norm1: Float = 0.0
        var norm2: Float = 0.0
        
        for i in 0..<embedding1.count {
            dotProduct += embedding1[i] * embedding2[i]
            norm1 += embedding1[i] * embedding1[i]
            norm2 += embedding2[i] * embedding2[i]
        }
        
        let magnitude = sqrt(norm1) * sqrt(norm2)
        guard magnitude > 0 else {
            return 0.0
        }
        
        return dotProduct / magnitude
    }
    
    /// –ù–∞—Ö–æ–¥–∏—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∑–∞–ø—Ä–æ—Å—É
    func findSimilarPhotos(query: String, limit: Int = 10) async -> [(Photo, Float)] {
        let queryEmbedding = await textToEmbedding(text: query)
        
        guard !queryEmbedding.isEmpty else {
            print("‚ùå Failed to generate query embedding")
            return []
        }
        
        var similarities: [(Photo, Float)] = []
        
        for photo in processedPhotos {
            let similarity = cosineSimilarity(queryEmbedding, photo.embedding)
            similarities.append((photo, similarity))
        }
        
        // –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return similarities.sorted { $0.1 > $1.1 }.prefix(limit).map { $0 }
    }

    private func processSingleAsset(_ asset: PHAsset) async -> Photo? {
        // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—Å—Å–µ—Ç –≤ –º–∏–Ω–∏–∞—Ç—é—Ä—É
        guard let thumbnail = await convertAssetToThumbnail(asset) else {
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–∏–Ω–∏–∞—Ç—é—Ä—É –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
            return nil
        }
        
        let embedding = await generateEmbedding(from: thumbnail)
        
        if embedding.isEmpty {
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–∏–Ω–≥ –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
            return nil
        }
        
        return Photo(asset: asset, embedding: embedding)
    }

    private func convertMultiArrayToFloatArray(_ multiArray: MLMultiArray) -> [Float] {
        let count = multiArray.count
        var result = [Float](repeating: 0, count: count)
        
        for i in 0..<count {
            result[i] = Float(truncating: multiArray[i])
        }
        
        return result
    }

    private func convertAssetToThumbnail(_ asset: PHAsset) async -> UIImage? {
        return await withCheckedContinuation { continuation in
            let imageManager = PHImageManager.default()
            let requestOptions = PHImageRequestOptions()
            
            requestOptions.isSynchronous = false
            requestOptions.deliveryMode = .highQualityFormat
            requestOptions.resizeMode = .exact
            requestOptions.isNetworkAccessAllowed = false
            
            // –†–∞–∑–º–µ—Ä –º–∏–Ω–∏–∞—Ç—é—Ä—ã –¥–ª—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
            let targetSize = CGSize(width: 224, height: 224)
            
            imageManager.requestImage(
                for: asset,
                targetSize: targetSize,
                contentMode: .aspectFill,
                options: requestOptions
            ) { image, info in
                continuation.resume(returning: image)
            }
        }
    }
}