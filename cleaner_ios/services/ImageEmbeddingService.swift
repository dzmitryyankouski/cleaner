import Foundation
import Vision
import CoreML
import UIKit
import Photos

struct Photo {
    let asset: PHAsset
    let embedding: [Float]
}

class ImageEmbeddingService {
    private var mobileClipModel: MLModel?
    private var concurrentTasks = 5

    var processedPhotos: [Photo] = []
    
    // Callback –¥–ª—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö
    var onPhotoProcessed: ((Photo) -> Void)?
    var onIndexingComplete: (() -> Void)?
    
    init() {
        guard Bundle.main.url(forResource: "mobileclip_s0_image", withExtension: "mlmodelc") != nil else {
            print("‚ùå Model not found in Bundle")
            return
        }

        if let modelURL = Bundle.main.url(forResource: "mobileclip_s0_image", withExtension: "mlmodelc") {
            do {
                mobileClipModel = try MLModel(contentsOf: modelURL)
            } catch {
                print("‚ùå Error loading model from Bundle: \(error)")
            }
        }
    }

    func indexPhotos(photos: [PHAsset]) async {
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é \(photos.count) —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π...")
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∞—Å—Å–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ç–∞—Å–∫–µ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º
        await withTaskGroup(of: Photo?.self) { group in
            var activeTasks = 0
            
            for asset in photos {
                // –ñ–¥–µ–º, –ø–æ–∫–∞ –æ—Å–≤–æ–±–æ–¥–∏—Ç—Å—è –º–µ—Å—Ç–æ –¥–ª—è –Ω–æ–≤–æ–π —Ç–∞—Å–∫–∏
                while activeTasks >= concurrentTasks {
                    if let result = await group.next() {
                        if let photo = result {
                            self.processedPhotos.append(photo)
                            // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                            await MainActor.run {
                                self.onPhotoProcessed?(photo)
                            }
                        }
                        activeTasks -= 1
                    }
                }
                
                group.addTask {
                    await self.processSingleAsset(asset)
                }
                activeTasks += 1
            }
            
            // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for await result in group {
                if let photo = result {
                    // –û–±–Ω–æ–≤–ª—è–µ–º UI –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                    await MainActor.run {
                        self.processedPhotos.append(photo)
                        // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
                        self.onPhotoProcessed?(photo)
                    }
                }
            }
        }
        
        // –£–≤–µ–¥–æ–º–ª—è–µ–º –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        await MainActor.run {
            self.onIndexingComplete?()
        }
    }

    func generateEmbedding(from image: UIImage) async -> [Float] {
        guard let mobileClipModel = mobileClipModel else {
            print("‚ùå Model not loaded")
            return []
        }

        guard let cgImage = image.cgImage else {
            print("‚ùå Error getting cgImage")
            return []
        }

        return await withCheckedContinuation { continuation in
            let request = VNCoreMLRequest(model: try! VNCoreMLModel(for: mobileClipModel)) { request, error in
                if let error = error {
                    print("‚ùå Error: \(error)")
                    continuation.resume(returning: [])
                    return
                }
                
                guard let results = request.results as? [VNCoreMLFeatureValueObservation],
                      let firstResult = results.first,
                      let multiArray = firstResult.featureValue.multiArrayValue else {
                    print("‚ùå Error getting embedding")
                    continuation.resume(returning: [])
                    return
                }
                
                let result = self.convertMultiArrayToFloatArray(multiArray)
                continuation.resume(returning: result)
            }
            
            request.imageCropAndScaleOption = VNImageCropAndScaleOption.centerCrop

            let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])

            DispatchQueue.global(qos: .userInitiated).async {
                do {
                    try handler.perform([request])
                } catch {
                    print("‚ùå Error: \(error)")
                    continuation.resume(returning: [])
                }
            }
        }
    }

    func generateEmbeddings(from images: [UIImage]) async -> [[Float]] {
        var results: [[Float]] = []
        
        for image in images {
            let embedding = await generateEmbedding(from: image)
            results.append(embedding)
        }
        
        return results
    }

    private func processSingleAsset(_ asset: PHAsset) async -> Photo? {
        do {
            // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∞—Å—Å–µ—Ç –≤ –º–∏–Ω–∏–∞—Ç—é—Ä—É
            guard let thumbnail = await convertAssetToThumbnail(asset) else {
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –º–∏–Ω–∏–∞—Ç—é—Ä—É –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
                return nil
            }
            
            let embedding = await generateEmbedding(from: thumbnail)
            
            if embedding.isEmpty {
                print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç–º–±–µ–¥–∏–Ω–≥ –¥–ª—è –∞—Å—Å–µ—Ç–∞: \(asset.localIdentifier)")
                return nil
            }
            
            return Photo(asset: asset, embedding: embedding)
            
        } catch {
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—Å—Å–µ—Ç–∞ \(asset.localIdentifier): \(error)")
            return nil
        }
    }

    private func convertMultiArrayToFloatArray(_ multiArray: MLMultiArray) -> [Float] {
        let count = multiArray.count
        var result = [Float](repeating: 0, count: count)
        
        for i in 0..<count {
            result[i] = Float(truncating: multiArray[i])
        }
        
        return result
    }

    private func convertAssetToThumbnail(_ asset: PHAsset) async -> UIImage? {
        return await withCheckedContinuation { continuation in
            let imageManager = PHImageManager.default()
            let requestOptions = PHImageRequestOptions()
            
            requestOptions.isSynchronous = false
            requestOptions.deliveryMode = .highQualityFormat
            requestOptions.resizeMode = .exact
            requestOptions.isNetworkAccessAllowed = false
            
            // –†–∞–∑–º–µ—Ä –º–∏–Ω–∏–∞—Ç—é—Ä—ã –¥–ª—è —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å)
            let targetSize = CGSize(width: 224, height: 224)
            
            imageManager.requestImage(
                for: asset,
                targetSize: targetSize,
                contentMode: .aspectFill,
                options: requestOptions
            ) { image, info in
                continuation.resume(returning: image)
            }
        }
    }
}